{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb362c8-9df3-4564-8989-159e18cef193",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Transformations and Actions\n",
    "\n",
    "Es gibt nur diese zwei verschiedene Operationen auf RDDs. Es ist wichtig, dass wir diese verstehen und auseinanderhalten können.\n",
    "\n",
    "## Transformationen\n",
    "\n",
    "* Eine Operation welche aus einem RDD einen neuen RDD erzeugt.\n",
    "* Sie bauen den Lineage Graph auf\n",
    "\n",
    "Bespiele sind:\n",
    "\n",
    "* Lesen einer Datei\n",
    "* Mappen einer Spalte in einen anderen Wert\n",
    "* Filter eines RDDs (nur Personen aus Hamburg)\n",
    "\n",
    "Sie werden **lazy** ausgeführt: erst wenn eine Action-Operation ausgeführt wird\n",
    "\n",
    "![](RDD-lineage-graph.dio.svg)\n",
    "\n",
    "## Actions\n",
    "\n",
    "* Gibt das Ergebniss von RDD-Anwendungen aus\n",
    "* Stößt die Anwendung von Transformationen über den Lineage-Graph an\n",
    "\n",
    "Bespiele sind:\n",
    "\n",
    "* Schreibe eine Ausgabe auf die Festplatte\n",
    "* Gebe die Ausgabe auf dem Bildschirm aus\n",
    "* Gib die Anzahl aus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a66de77-e72c-49e9-ab3b-e17fd2f39925",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/lib/python3.10/dist-packages/pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f7bd87-9fe7-403f-8813-8ab947c687a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/09 00:58:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pupil-a.bin-ich-tot.de:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jobs-stages-tasks</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd96df6c6d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"RDD-Basic-Operations\")\n",
    "        .master(\"local[4]\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834894b2-b84f-4424-9999-6fad709fcb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# erzeuge RDD mit Hilfe des Spark Kontext\n",
    "\n",
    "numbers_rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc19f8-fa40-4374-b8c0-0083f7baf593",
   "metadata": {},
   "source": [
    "Wurde die Operation wirklich ausgeführt?\n",
    "\n",
    "Nein: sie wurde nur dem Lineage Graph hinzugefügt.\n",
    "\n",
    "Wir können das in der Spark-UI überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56bacbf6-f89f-4929-a964-f639a59d0eec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nun geben wir den RDD aus\n",
    "\n",
    "numbers_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20bb0e-14b5-48a8-a06c-ae842dae280d",
   "metadata": {},
   "source": [
    "In der Spark-UI sehen wir nun **1** Job und **4** Tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9e715a5-10a1-4d5f-b7e5-ab1cd1de9862",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nun das erste Element ausgeben\n",
    "\n",
    "numbers_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639c6cc-aa3e-43ea-9811-4ec6170eb4ed",
   "metadata": {},
   "source": [
    "Wir sehen in der Spark UI einen neuen Job mit genau einem Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8162e45-5948-427f-b2d6-803c891da69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nun führen wir mehere Operationen hintereinander aus\n",
    "\n",
    "taxi_zones_rdd = sc.textFile(\"TaxiZones.csv\")\n",
    "\n",
    "# aufsplitten\n",
    "taxi_zones_with_cols_rdd = taxi_zones_rdd.map(lambda zone: zone.split(\",\"))\n",
    "\n",
    "# Pair RDD über die Bezirke\n",
    "\n",
    "taxi_zones_by_bezirk = taxi_zones_with_cols_rdd.map(lambda zone: (zone[1],1))\n",
    "\n",
    "# aufsummieren\n",
    "taxi_zones_count_bezirke = taxi_zones_by_bezirk.reduceByKey( lambda a, b: a + b)\n",
    "\n",
    "# filtern\n",
    "\n",
    "only_big_bezirke = taxi_zones_count_bezirke.filter( lambda row: row[1] > 10 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcab54e6-f201-4a47-83c2-de9f168edf63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Manhattan', 69),\n",
       " ('Brooklyn', 61),\n",
       " ('Queens', 69),\n",
       " ('Bronx', 43),\n",
       " ('Staten Island', 20)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_big_bezirke.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4099d0-3a1c-4804-9be4-494d3ae9d2f8",
   "metadata": {},
   "source": [
    "Nun in der Spark-UI prüfen was passiert ist..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a853f-5365-472e-a44f-6da7ac6501f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Narrow Transformations\n",
    "\n",
    "Wir erinnern uns: es gibt zwei Arten von Transformationen. Narrow und Wide. Wir schauen und nun Narrow Transformations an.\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Eine Narrow Transformation ist eine Transformation mit der für eine Output-Partition nur genau eine Eingabe Partition genutzt wird.\n",
    "\n",
    "![](RDD-narrow.dio.svg)\n",
    "\n",
    "Beispiele:\n",
    "* Map\n",
    "* MapPartition\n",
    "* Filter\n",
    "* Sample\n",
    "* Union\n",
    "* ...\n",
    "\n",
    "**Eigenschaften**\n",
    "\n",
    "* Die Zahl der Eingabe Partitionen ist in der Regel gleich der der Ausgabe-Partitionen.\n",
    "* Sehr schnell. Zwischen den Partitionen werden keine \n",
    "\n",
    "## Wide Transformation\n",
    "\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Eine Wide-Transformationen besteht aus zwei Schritten und führt eine sogenannte Shuffle-Partition ein.\n",
    "\n",
    "![](RDD-wide.dio.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1422ed23-2a1b-4089-8ee7-c10e6687df2a",
   "metadata": {},
   "source": [
    "# Jobs, Stages and Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63186e5d-2fc4-4131-b0bb-599bda5b9003",
   "metadata": {},
   "source": [
    "## Setting up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b50fea5-886e-411c-b593-9f6822ed078a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/08 18:00:52 WARN Utils: Your hostname, pupil-a resolves to a loopback address: 127.0.1.1; using 167.235.141.210 instead (on interface eth0)\n",
      "23/09/08 18:00:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/08 18:00:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/08 18:00:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/08 18:00:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pupil-a.bin-ich-tot.de:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jobs-stages-tasks</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd96df6c6d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"jobs-stages-tasks\").master(\"local[4]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e128cb-dff0-4124-8d89-b819bef1e718",
   "metadata": {},
   "source": [
    "## Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48230e51-1247-4045-a3ee-a09032f2ec42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,EWR,Newark Airport,EWR',\n",
       " '2,Queens,Jamaica Bay,Boro Zone',\n",
       " '3,Bronx,Allerton/Pelham Gardens,Boro Zone',\n",
       " '4,Manhattan,Alphabet City,Yellow Zone',\n",
       " '5,Staten Island,Arden Heights,Boro Zone',\n",
       " '6,Staten Island,Arrochar/Fort Wadsworth,Boro Zone',\n",
       " '7,Queens,Astoria,Boro Zone',\n",
       " '8,Queens,Astoria Park,Boro Zone',\n",
       " '9,Queens,Auburndale,Boro Zone',\n",
       " '10,Queens,Baisley Park,Boro Zone']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones_with_cols_rdd = sc.textFile(\"TaxiZones.csv\", 4)\n",
    "zones_with_cols_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6e1fc-d4aa-48a1-9046-ae634942ec63",
   "metadata": {},
   "source": [
    "Überlege wieviele\n",
    " * Jobs\n",
    " * Transformationen\n",
    " * Actions\n",
    " * Stages\n",
    "erzeugt wurden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496f3fa-b78a-4e0d-9d21-a66a37b73732",
   "metadata": {},
   "source": [
    "## Reading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b0cc1f-a0f6-4ccc-be72-dcfe9e0aaec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'EWR', 'Newark Airport', 'EWR'],\n",
       " ['2', 'Queens', 'Jamaica Bay', 'Boro Zone'],\n",
       " ['3', 'Bronx', 'Allerton/Pelham Gardens', 'Boro Zone'],\n",
       " ['4', 'Manhattan', 'Alphabet City', 'Yellow Zone'],\n",
       " ['5', 'Staten Island', 'Arden Heights', 'Boro Zone'],\n",
       " ['6', 'Staten Island', 'Arrochar/Fort Wadsworth', 'Boro Zone'],\n",
       " ['7', 'Queens', 'Astoria', 'Boro Zone'],\n",
       " ['8', 'Queens', 'Astoria Park', 'Boro Zone'],\n",
       " ['9', 'Queens', 'Auburndale', 'Boro Zone'],\n",
       " ['10', 'Queens', 'Baisley Park', 'Boro Zone']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading into 4 partitions \n",
    "zones_rdd = sc.textFile(\"TaxiZones.csv\", 4)\n",
    "# Splitting each row by ,\n",
    "zones_with_cols_rdd = zones_rdd.map(lambda x: x.split(\",\"))\n",
    "\n",
    "zones_with_cols_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b15e134e-c5ea-4ab8-aaee-7f88633db116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading the file: 4\n",
      "After applying map: 4\n"
     ]
    }
   ],
   "source": [
    "# Hat sich die Anzahl der Partitionen im Laufe der Transformationen verändert?\n",
    "\n",
    "print(\"After reading the file: \" + str(zones_rdd.getNumPartitions()))\n",
    "print(\"After applying map: \" + str(zones_with_cols_rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae8e89-db83-48de-ae0f-e5ccceff4109",
   "metadata": {},
   "source": [
    "## Reading and creating a pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fb5df36-065c-4687-8c6c-e649cd7d98d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 1), ('2', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading into 4 partitions \n",
    "zones_rdd = sc.textFile(\"TaxiZones.csv\", 4)\n",
    "# Splitting each row by ,\n",
    "zones_with_cols_rdd = zones_rdd.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# Now creating a pair RDD for counting the amount of boroughs\n",
    "zones_pair_rdd = zones_with_cols_rdd.map(lambda x: (x[0],1))\n",
    "\n",
    "print(zones_rdd.count())\n",
    "print(zones_with_cols_rdd.count())\n",
    "print(zones_pair_rdd.count())\n",
    "\n",
    "zones_with_cols_rdd.take(10)\n",
    "\n",
    "zones_pair_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f84ba-bcba-4c33-8c7e-73dda3ec3e8d",
   "metadata": {},
   "source": [
    "### Now also finding distinct records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551c7312-3fe4-4001-b5d4-bee34e379b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Bronx', 1),\n",
       " ('Staten Island', 1),\n",
       " ('Queens', 1),\n",
       " ('EWR', 1),\n",
       " ('Manhattan', 1),\n",
       " ('Brooklyn', 1),\n",
       " ('Unknown', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading into 4 partitions \n",
    "zones_rdd = sc.textFile(\"TaxiZones.csv\", 4)\n",
    "# Splitting each row by ,\n",
    "zones_with_cols_rdd = zones_rdd.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# Now creating a pair RDD for counting the amount of boroughs\n",
    "zones_pair_rdd = zones_with_cols_rdd.map(lambda x: (x[1],1))\n",
    "\n",
    "distinct_zones_pair_rdd = zones_pair_rdd.distinct()\n",
    "\n",
    "print(zones_rdd.count())\n",
    "print(zones_with_cols_rdd.count())\n",
    "print(zones_pair_rdd.count())\n",
    "\n",
    "distinct_zones_pair_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "567f4235-db94-484e-87df-f7afad3bc894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Bronx', 43),\n",
       " ('Staten Island', 20),\n",
       " ('EWR', 1),\n",
       " ('Manhattan', 69),\n",
       " ('Brooklyn', 61),\n",
       " ('Unknown', 2),\n",
       " ('Queens', 69)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading into 4 partitions \n",
    "zones_rdd = sc.textFile(\"TaxiZones.csv\", 4)\n",
    "# Splitting each row by ,\n",
    "zones_with_cols_rdd = zones_rdd.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# Now creating a pair RDD for counting the amount of boroughs\n",
    "zones_pair_rdd = zones_with_cols_rdd.map(lambda x: (x[1],1))\n",
    "\n",
    "counted = zones_pair_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "counted.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf83e8c0-eec9-40aa-9ae0-491afd41d835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones_pair_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23995b74-6325-4408-bedd-1491a3c4bb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
