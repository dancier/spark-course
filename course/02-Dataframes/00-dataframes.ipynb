{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976b4130-4eba-4c43-bc37-cb14652f9332",
   "metadata": {},
   "source": [
    "Einleitung\n",
    "\n",
    "higher level api\n",
    "part of spark-sql lib\n",
    "build on top of rdd\n",
    " * in memory\n",
    " * partitioned\n",
    " * ready-only\n",
    " * resilient\n",
    "\n",
    "Tabllenstruktur ähnlich relationalen Datenbanken\n",
    "\n",
    "Spark wendet eine Reihe von Optimierungen an welche die Entwicklung verglichen mit RDD verkürzen und die Geschwindigkeit erhöhen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1038a2-70f6-45fa-9edc-9ef20f851e57",
   "metadata": {},
   "source": [
    "## [Catalyst Optimizer](https://www.databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)\n",
    "\n",
    "Query Optimizer to produce an optimized query plan\n",
    "\n",
    "1. Analysis\n",
    "2. Logical Optimization\n",
    "3. Physical Planning\n",
    "4. Code Generation\n",
    "\n",
    "Let's have a look at a concrete Example\n",
    "\n",
    "Nehmen wir an wir haben Code mit Dataframe API oder SQL.\n",
    "Als erstes wird dieser Code analysiert:\n",
    "\n",
    "### Analyse-Phase\n",
    "\n",
    "* Syntax Check\n",
    "* Unresolved logical Plan\n",
    "  \n",
    "  hier wird z.B. noch nicht geprüft ob Spalten existieren oder den richtigen Typ haben\n",
    "* Nun wird eben diese mit Hilfe des Spark Catalogs geprüft und der Logische Plan erzeugt.\n",
    "\n",
    "### Optimierung des logischen Planes\n",
    "\n",
    "* Regelbasierte Optimierungen\n",
    "  * [Predicate Pushdown](https://medium.com/microsoftazure/data-at-scale-learn-how-predicate-pushdown-will-save-you-money-7063b80878d7)\n",
    "  * [Constant folding](https://en.wikipedia.org/wiki/Constant_folding)\n",
    "\n",
    "Das ended im optimierten logischen Plan.\n",
    "\n",
    "### Physical Plan\n",
    "\n",
    "Für den optimierten logsichen Plan werden ein Reihe von Physischen Plänen berechnet und mit einem Kostenmodell bewertet. Der günstige wird dann genommen.\n",
    "\n",
    "### Code Generation Phase\n",
    "\n",
    "Am Ende sind es wieder RDDs die in Java Bytecode überführt werden.\n",
    "\n",
    "### Beachte\n",
    "\n",
    "all diese Optimierungen bekommst Du nicht wenn Du direkt RDD Code schreibst. Diese müsstest Du selber ausprogrammieren.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd6f932-8acc-4349-bb24-6a19687713c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/28 14:01:40 WARN Utils: Your hostname, pupil-a resolves to a loopback address: 127.0.1.1; using 167.235.141.210 instead (on interface eth0)\n",
      "23/08/28 14:01:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/28 14:01:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/28 14:01:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pupil-a.bin-ich-tot.de:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>catalyst</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f132ae18d30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"catalyst\").master(\"local[4]\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4792dce-b826-47e8-ac79-bb903b6ac325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PULocationID|count|\n",
      "+------------+-----+\n",
      "|         148|38049|\n",
      "|         243|  514|\n",
      "|          31|   22|\n",
      "|         137|43171|\n",
      "|         251|    6|\n",
      "|          85|   95|\n",
      "|          65| 3042|\n",
      "|         255| 1969|\n",
      "|          53|   41|\n",
      "|         133|   92|\n",
      "|          78|  113|\n",
      "|         155|  127|\n",
      "|         108|   78|\n",
      "|         211|26838|\n",
      "|         193| 3383|\n",
      "|          34|  113|\n",
      "|         126|   67|\n",
      "|         101|   50|\n",
      "|         115|    5|\n",
      "|          81|   73|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "taxi_rides = spark.read.csv(\"YellowTaxis_202210.csv.gz\", header = True, inferSchema=True)\n",
    "taxi_rides_by_location = taxi_rides.groupby(\"PULocationID\").count()\n",
    "taxi_rides_81 = taxi_rides_by_location.where(\"PULocationID == 81\")\n",
    "taxi_rides_by_location.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08d2cb-aac6-4030-b250-bd67bf06ea08",
   "metadata": {},
   "source": [
    "# Bild einfügen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cfb76a-588f-4ec1-b0e4-903fc33d1429",
   "metadata": {},
   "source": [
    "## [Tungsten Engine](https://www.databricks.com/glossary/tungsten)\n",
    "\n",
    "Improves efficiency of memory & CPU for spark applications during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce9867-75a2-425a-b07d-248b6c2c26f4",
   "metadata": {},
   "source": [
    "## Performance Vergleich\n",
    "\n",
    "von der Datebricks Catalyst Seite einblenden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee8024-b049-4b0d-8e24-ab0f52bd8529",
   "metadata": {},
   "source": [
    "Learning: wenn Du die Dataframe API nutzt, dann ist es egal ob Du Python/Scala/SQL nutzt, der Code ist immer gleich und nahezu so schnell wie handoptimierter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7d784-2fff-461a-a6c9-52538289e7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
