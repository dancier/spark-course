{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c9d657-e97c-45b9-b132-4f9a1e817c09",
   "metadata": {},
   "source": [
    "# SQL, Spark, Tables, Reporting ...\n",
    "\n",
    "* SQL Abfragen auf Dataframes anwenden\n",
    "* Mit Spark Datenbanken & Tabellen arbeiten\n",
    "* User-Defined Functions benutzen\n",
    "* Opertionen auf mehrere Datenmegen gleichzeitig anwenden\n",
    "* Kleiner Deep Dive auf Windmow Operationen\n",
    "* Testdaten erzeugen die zwei Geschäftsprozesse aus der Versicherungswelt abbilden\n",
    "* Die ersten (und wesentlichsten) Schritte für ein sinnvolles Monitoring dieser Prozesse gehen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb03304-9de0-40e1-b3ac-843e16c0ef9c",
   "metadata": {},
   "source": [
    "## Wie immer eine Spark-Application registieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1b904ba-baea-447b-86a5-0640e887ee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/lib/python3.10/dist-packages/pyspark'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28fd1b01-e8cb-4f68-b89c-4ecce0c5d0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pupil-a.bin-ich-tot.de:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sql</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f021c165ea0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"sql\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\",False)\n",
    "        .config(\"spark.sql.adaptive.enabled\",False)\n",
    "        .master(\"local[4]\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb782484-df8a-4887-85b3-ff1a071b3f85",
   "metadata": {},
   "source": [
    "## Nun Testdaten erzeugen\n",
    "Hierzu haben wir ein Beispiel aus der Versicherungswelt vorbereiten. Vielleicht ist es dem in der Signal sogar ein bisschen ähnlich ;-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe6873-4e84-43c4-9ebe-80ada0d84831",
   "metadata": {},
   "source": [
    "![](architecture-level-01.dio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39e73784-9780-4b5c-a47b-30e6cdfdf20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_test_data\n",
    "generate_test_data.main(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467f314-57c1-498e-9a7f-e91e5350e433",
   "metadata": {},
   "source": [
    "Wir sehen nun auf im Dateibrowser eine Liste von Dateien, die überwiegend fachliche Events enthalten, die zwei Geschäftsprozesse abbilden:\n",
    "\n",
    "### Ein Kunden bekommt einen neuen Vertrag\n",
    "\n",
    "Die Systeme welche die Events schreiben liegen in der Verantwortung verschiedener Teams:\n",
    "\n",
    "#### Team Antrag\n",
    " * **Antrag Erzeugt:** Der Kunde geht im Internet auf ein System um einen Antrag auf einen Versicherungsvertrag zu stellen. Dieses System validert schon ein wenig, versucht einen schon bestehenden Kunden im Partner-Systeme zu finden und schmeißt im Anschluss dieses Business-Event.\n",
    " * **AB-Test:** das Team Antrag ist stehst bemüht die User-Experience zu optimieren. So will es zum Beispiel die Durchlaufzeit auf ihrem eigenen System zu verkürzen. So variert es z.B. die Farbe des Knopfes. Wenn es bei einem Antrag von der Standardfarbe des Knopfes abweicht, schreibt es dieses technische Event\n",
    " * **Kunde hat Angebot aktzeptiert:** Nch\n",
    " * **Kunde hat Angebot abgelehnt:**\n",
    " \n",
    "#### Team Vertrag\n",
    " * **Antrag abgelehnt**: Ein Vertrag kann vom Vertragssystem abgelehnt werden. Dieses fragt z.B. noch bei Fraud an und validert auch sonst den Antrag viel \"besser\" als es das Antragssystem könnte.\n",
    " * **Kunde angelegt**: (eigentlich müsste ein anderes System dieses Event schreiben, aber in unserem Beispiel soll das mal OK sein) Wenn in einem Antrag nicht schon eine Referenz auf einen Kunden vorhanden ist, dann legt das Vertragssystem einen Kunden an und informiert Gott und die Welt hierüber.\n",
    " * **Vertrag angeboten**: Wenn ein Vertragsantrag aus Sicht des Vertragssystem gut aussieht, dann gibt es den Vertrag frei zur Annahme durch den Kunden. \n",
    " * **Vertrag policiert**: Wenn der Kunden ein Vertragsangebot akzeptiert hat, policieren wir den Vertrag. Nun ist der Kunde versichert. Jetzt können bedenkenlos Schadensfälle eintreten.\n",
    " \n",
    "#### Team Schaden\n",
    " * **Schaden reguliert**: wenn ein Schadensfall geprüft wurde und alles passt, dann überweisen wir Geld\n",
    "\n",
    "#### Übrigens\n",
    "**kunden.csv** ist nur ein Container mit allen Kunden ;-)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeeb418-98c1-436d-b13f-d319c4420962",
   "metadata": {},
   "source": [
    "## Erste Blicke in unsere Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0bcb483-6bc7-4786-9171-f5fad2a699fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------------------------------------+-------------------+\n",
      "|id                                  |name               |\n",
      "+------------------------------------+-------------------+\n",
      "|ce4cf720-4a1a-11ee-8349-7b31dfcc7252|gorzala            |\n",
      "|d55a2ede-4a1a-11ee-8cce-b739d8c6e256|Scharrenberch      |\n",
      "|bcecc0dc-af06-4910-b22e-12030333ab5b|Agitated Williamson|\n",
      "|d19506f3-4558-4b3c-9e01-938700866ae9|Crazy Villani      |\n",
      "+------------------------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kunden_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"kunden.csv\")\n",
    "kunden_df.printSchema()\n",
    "kunden_df.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb40d3f-36c3-4e93-8b5a-0925cead5fc2",
   "metadata": {},
   "source": [
    "Soweit so bekannt. Wir könnten jetzt auf die Daten zugreifen in dem wir wie in den vorhergegangen Kapiteln filter anwenden, Null-Werte entfernen usw.\n",
    "Wir möchten aber absofort ganz normal mit SQL auf unsere Daten zugreifen. Dazu müssen wir zu einem Dataframe in Spark einen _View_ anlegen. Diese Views werden dann ganz normal wie Datenbanktabellen in SQL-Datenbanken behandelt (read-only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5542e6b6-4131-4e40-8cc0-cc59eb30c4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunden_df.createOrReplaceTempView(\"kunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd663dc-7343-45ee-a11d-d4c239282abd",
   "metadata": {},
   "source": [
    "**Beachte:** die Funtion gibt nichts zurück. In der Spark-Application ist jetzt eine eine Tabelle unter dem Namen \"kunden\" verfügbar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29602ac9-7eec-46ca-8a78-d388b378bf57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, name: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM kunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f3882-06ed-48bd-9a47-f15395c21d2c",
   "metadata": {},
   "source": [
    "Wir sehen das wir ein Dataframe zurückbekommen. Auf diesem können wir dann wieder die bekannten Operationen ausführen. Beachte, ob Du die Dateframe-API oder die SQL-API nutzt, hat in der Regel **keinen** Impact auf die Performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e8898-cb49-4cbc-a10d-6dd4515e8ea8",
   "metadata": {},
   "source": [
    "**Übung** setzte hier mal ein paar SQL gegen diese Tabelle ab. Filter nach bestimmten Namen... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3df4036e-3276-4df3-b507-8ac26ae1bf94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  id|                name|\n",
      "+--------------------+--------------------+\n",
      "|ce4cf720-4a1a-11e...|             gorzala|\n",
      "|d55a2ede-4a1a-11e...|       Scharrenberch|\n",
      "|bcecc0dc-af06-491...| Agitated Williamson|\n",
      "|d19506f3-4558-4b3...|       Crazy Villani|\n",
      "|0ee2f173-8860-452...|Compassionate Noe...|\n",
      "|2cf46a78-d800-48e...|       Busy Sinoussi|\n",
      "|292e6eda-4dab-4f7...|    Wizardly Wozniak|\n",
      "|4aa7e4ad-bb89-4c4...| Wonderful Mendeleev|\n",
      "|14272797-7ddf-451...|      Great Brattain|\n",
      "|5a80abe6-594e-4a0...|    Strange Roentgen|\n",
      "|93cfdc26-9feb-4c6...|     Determined Borg|\n",
      "|7049985c-95d9-430...|    Gracious Wescoff|\n",
      "|f1573be3-8c18-4c7...|         Brave Morse|\n",
      "|4e5449a8-4fed-48b...|      Elegant Agnesi|\n",
      "|10107419-c873-41a...|   Beautiful Vaughan|\n",
      "|0c25d304-df3a-45c...|       Elegant Kilby|\n",
      "|1a0d4987-7faf-47c...|  Affectionate Gates|\n",
      "|90127df4-9eda-47f...|      Zealous Pascal|\n",
      "|2a45e83f-f90e-42b...|          Bold Gauss|\n",
      "|89370b2b-23b7-4f2...|      Romantic Curie|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM kunden').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879e92f-a73a-431f-a7ba-71ac0461eba6",
   "metadata": {},
   "source": [
    "## Alle Testdaten importieren\n",
    "und views anlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "561abcf9-a58a-42e1-b2e3-857e7a7400f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AB-test\n",
    "ab_test_schema = \"AntragsId STRING\"\n",
    "ab_test_df = spark.read.option(\"header\", True).schema(ab_test_schema).csv(\"ab_test.csv\")\n",
    "ab_test_df.createOrReplaceTempView(\"ab_test\")\n",
    "\n",
    "# Antrag Abgelehnt\n",
    "antrag_abgelehnt_schema = \"AntragsId STRING, KundenId STRING, TimeStamp TIMESTAMP, Grund STRING\"\n",
    "antrag_abgelehnt_df = spark.read.option(\"header\", True).schema(antrag_abgelehnt_schema).csv(\"antrag_abgelehnt.csv\")\n",
    "antrag_abgelehnt_df.createOrReplaceTempView(\"antrag_abgelehnt\")\n",
    "\n",
    "# Antrag Erzeugt\n",
    "antrag_erzeugt_schema = \"AntragsId STRING, StartZeit TIMESTAMP, EndZeit TIMESTAMP, KundenId STRING\"\n",
    "antrag_erzeugt_df = spark.read.option(\"header\", True).schema(antrag_erzeugt_schema).csv(\"antrag_erzeugt.csv\")\n",
    "antrag_erzeugt_df.createOrReplaceTempView(\"antrag_erzeugt\")\n",
    "\n",
    "# Kunde Angelegt\n",
    "kunde_angelegt_schema = \"KundenId STRING, TimeStamp TIMESTAMP\"\n",
    "kunde_angelegt_df = spark.read.option(\"header\", True).schema(kunde_angelegt_schema).csv(\"kunde_angelegt.csv\")\n",
    "kunde_angelegt_df.createOrReplaceTempView(\"kunde_angelegt\")\n",
    "\n",
    "# Kunde hat Angebot Abgelehnt\n",
    "kunde_hat_angebot_abgelehnt_schema = \"VertragsId STRING, AntragsId STRING, Grund STRING, TimeStamp TIMESTAMP\"\n",
    "kunde_hat_angebot_abgelehnt_df = spark.read.option(\"header\", True).schema(kunde_hat_angebot_abgelehnt_schema).csv(\"kunde_hat_angebot_abgelehnt.csv\")\n",
    "kunde_hat_angebot_abgelehnt_df .createOrReplaceTempView(\"kunde_hat_angebot_abgelehnt\")\n",
    "\n",
    "# Kunde hat Angebot Akzeptiert\n",
    "kunde_hat_angebot_akzeptiert_schema = \"VertragsId STRING, AntragsId STRING, KundenId STRING, TimeStamp TIMESTAMP\"\n",
    "kunde_hat_angebot_akzeptiert_df = spark.read.option(\"header\", True).schema(kunde_hat_angebot_akzeptiert_schema).csv(\"kunde_hat_angebot_akzeptiert.csv\")\n",
    "kunde_hat_angebot_akzeptiert_df.createOrReplaceTempView(\"kunde_hat_angebot_akzeptiert\")\n",
    "\n",
    "# Schaden Gemeldet\n",
    "schaden_gemeldet_schema = \"VertagsId STRING, SchadensId STRING, Schadenshoehe INT, TimeStamp TIMESTAMP\"\n",
    "schaden_gemeldet_df = spark.read.option(\"header\", True).schema(schaden_gemeldet_schema).csv(\"schaden_gemeldet.csv\")\n",
    "schaden_gemeldet_df.createOrReplaceTempView(\"schaden_gemeldet\")\n",
    "\n",
    "# Schaden Reguliert\n",
    "schaden_reguliert_schema = \"SchadensId STRING, TimeStamp TIMESTAMP\"\n",
    "schaden_reguliert_df = spark.read.option(\"header\", True).schema(schaden_reguliert_schema).csv(\"schaden_reguliert.csv\")\n",
    "schaden_reguliert_df.createOrReplaceTempView(\"schaden_reguliert\")\n",
    "\n",
    "# Vertrag Angeboten\n",
    "vertrag_angeboten_schema = \"VertragsId STRING, AntragsId STRING, KundenId STRING, TimeStamp TIMESTAMP\"\n",
    "vertrag_angeboten_df = spark.read.option(\"header\", True).schema(vertrag_angeboten_schema).csv(\"vertrag_angeboten.csv\")\n",
    "vertrag_angeboten_df.createOrReplaceTempView(\"vertrag_angeboten\")\n",
    "\n",
    "# Vertrag Policiert\n",
    "vertrag_policiert_schema = \"VertragsId STRING, TimeStamp TIMESTAMP\"\n",
    "vertrag_policiert_df = spark.read.option(\"header\", True).schema(vertrag_policiert_schema).csv(\"vertrag_policiert.csv\")\n",
    "vertrag_policiert_df.createOrReplaceTempView(\"vertrag_policiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "443150a9-ae9f-43a0-b299-f7f49f157123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------+-----------+\n",
      "|namespace|tableName                   |isTemporary|\n",
      "+---------+----------------------------+-----------+\n",
      "|         |ab_test                     |true       |\n",
      "|         |antrag_abgelehnt            |true       |\n",
      "|         |antrag_erzeugt              |true       |\n",
      "|         |kunde_angelegt              |true       |\n",
      "|         |kunde_hat_angebot_abgelehnt |true       |\n",
      "|         |kunde_hat_angebot_akzeptiert|true       |\n",
      "|         |kunden                      |true       |\n",
      "|         |schaden_gemeldet            |true       |\n",
      "|         |schaden_reguliert           |true       |\n",
      "|         |vertrag_angeboten           |true       |\n",
      "|         |vertrag_policiert           |true       |\n",
      "+---------+----------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alles da?\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc50f92f-8c67-4938-a6d5-768ba374cf36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|    VertagsId|   string|   null|\n",
      "|   SchadensId|   string|   null|\n",
      "|Schadenshoehe|      int|   null|\n",
      "|    TimeStamp|timestamp|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stichprobe bzgl. der Datentypen?\n",
    "spark.sql(\"DESCRIBE schaden_gemeldet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3661e5a-a183-41b1-92f1-50fc718b3618",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ein erstes Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2e5d6f0-0e58-40f4-9780-6d9aafee0ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3000|\n",
      "+--------+\n",
      "\n",
      "+--------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "|           AntragsId|          StartZeit|            EndZeit|            KundenId|           AntragsId|\n",
      "+--------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "|5951a2fe-9298-44f...|2023-09-12 03:24:24|2023-09-12 03:28:24|292e6eda-4dab-4f7...|5951a2fe-9298-44f...|\n",
      "|97fc3fac-8a15-419...|2023-09-11 00:28:38|2023-09-11 00:29:38|                null|97fc3fac-8a15-419...|\n",
      "|fd99d05e-87f6-4a9...|2023-09-16 20:32:41|2023-09-16 20:35:41|                null|fd99d05e-87f6-4a9...|\n",
      "|afc546ba-0430-4af...|2023-09-02 21:47:10|2023-09-02 21:49:10|6acf9d4f-fe9b-4be...|afc546ba-0430-4af...|\n",
      "|8f942606-be75-475...|2023-09-03 14:09:28|2023-09-03 14:11:28|7e2105ed-8c7b-46b...|8f942606-be75-475...|\n",
      "|4fbbbee7-fff5-4ae...|2023-09-13 04:35:39|2023-09-13 04:36:39|b2f8a2e6-8e0d-4e4...|4fbbbee7-fff5-4ae...|\n",
      "|0abd263c-87a0-444...|2023-09-11 16:42:13|2023-09-11 16:43:13|                null|0abd263c-87a0-444...|\n",
      "|eebb0ca8-5788-417...|2023-09-19 15:02:52|2023-09-19 15:06:52|9b32bc1a-b70b-4fb...|eebb0ca8-5788-417...|\n",
      "|790a6cc1-d43d-4b3...|2023-09-16 11:46:19|2023-09-16 11:49:19|b3064560-de6d-4e9...|790a6cc1-d43d-4b3...|\n",
      "|b014a28e-ae27-498...|2023-09-04 21:50:24|2023-09-04 21:53:24|ac5bb82d-1bc7-424...|b014a28e-ae27-498...|\n",
      "|aa26ea84-aa8b-4af...|2023-09-12 16:31:09|2023-09-12 16:34:09|da5b6688-3cad-430...|aa26ea84-aa8b-4af...|\n",
      "|6d22c275-a139-441...|2023-09-03 13:38:18|2023-09-03 13:41:18|9b1a4886-577d-41e...|6d22c275-a139-441...|\n",
      "|8daac755-86d3-4fd...|2023-09-09 04:32:18|2023-09-09 04:36:18|ef8965f8-17c6-4b6...|8daac755-86d3-4fd...|\n",
      "|3859158e-3118-415...|2023-09-15 07:02:24|2023-09-15 07:06:24|b95d4d4a-a253-45e...|3859158e-3118-415...|\n",
      "|536867fb-c8f1-45b...|2023-09-09 23:04:20|2023-09-09 23:07:20|ef69621a-00e9-4c6...|536867fb-c8f1-45b...|\n",
      "|5a93c623-98f7-4b1...|2023-09-09 00:41:02|2023-09-09 00:45:02|fb89d694-896c-45a...|5a93c623-98f7-4b1...|\n",
      "|190cc6fd-f700-46f...|2023-09-03 12:11:55|2023-09-03 12:15:55|af40fcd6-3a66-4a2...|190cc6fd-f700-46f...|\n",
      "|7bbb36d1-bb15-41f...|2023-09-08 10:09:02|2023-09-08 10:11:02|c7911dcc-5a02-406...|7bbb36d1-bb15-41f...|\n",
      "|e76f0c2e-086e-4e7...|2023-09-09 09:46:58|2023-09-09 09:47:58|b654605b-95cd-488...|e76f0c2e-086e-4e7...|\n",
      "|aed6c211-5b27-483...|2023-09-19 17:15:05|2023-09-19 17:18:05|                null|aed6c211-5b27-483...|\n",
      "+--------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zeige alle Anträge an, bei den wir die Farbe des Knopfs verändert haben\n",
    "\n",
    "# erstmal alle Anträge\n",
    "\n",
    "spark.sql(\"SELECT count(1) FROM antrag_erzeugt\").show()\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "      FROM antrag_erzeugt\n",
    " INNER JOIN ab_test\n",
    "        ON antrag_erzeugt.AntragsId = ab_test.AntragsId\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92569e-9242-449c-be17-ac431fbe03a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
