{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c9d657-e97c-45b9-b132-4f9a1e817c09",
   "metadata": {},
   "source": [
    "# Der Höhepunkt: SQL, Spark-Tables, Reporting ...\n",
    "\n",
    "* SQL Abfragen auf Dataframes anwenden\n",
    "* Mit Spark Datenbanken & Tabellen arbeiten\n",
    "* User-Defined Functions benutzen\n",
    "* Datenmegen joinen\n",
    "* Window Operationen\n",
    "* Testdaten erzeugen, welche zwei Geschäftsprozesse aus der Versicherungswelt abbilden\n",
    "* Die ersten (und wesentlichsten) Schritte für ein sinnvolles Monitoring dieser Prozesse gehen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb03304-9de0-40e1-b3ac-843e16c0ef9c",
   "metadata": {},
   "source": [
    "## Wie immer eine Spark-Application registieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b904ba-baea-447b-86a5-0640e887ee74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd1b01-e8cb-4f68-b89c-4ecce0c5d0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"sql\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\",False)\n",
    "        .config(\"spark.sql.adaptive.enabled\",False)\n",
    "        .master(\"local[4]\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb782484-df8a-4887-85b3-ff1a071b3f85",
   "metadata": {},
   "source": [
    "## Nun Testdaten erzeugen\n",
    "Hierzu haben wir ein Beispiel aus der Versicherungswelt vorbereiten. Vielleicht ist es dem in der Signal sogar ein bisschen ähnlich ;-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe6873-4e84-43c4-9ebe-80ada0d84831",
   "metadata": {},
   "source": [
    "![](architecture-level-01.dio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e73784-9780-4b5c-a47b-30e6cdfdf20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import generate_test_data\n",
    "generate_test_data.main(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467f314-57c1-498e-9a7f-e91e5350e433",
   "metadata": {
    "tags": []
   },
   "source": [
    "Wir sehen nun auf im Dateibrowser eine Liste von Dateien, die überwiegend fachliche Events enthalten, die zwei Geschäftsprozesse abbilden:\n",
    "\n",
    "### Ein Kunden bekommt einen neuen Vertrag\n",
    "\n",
    "Die Systeme welche die Events schreiben liegen in der Verantwortung verschiedener Teams:\n",
    "\n",
    "#### Team Antrag\n",
    " * **Antrag Erzeugt:** Der Kunde geht im Internet auf ein System um einen Antrag auf einen Versicherungsvertrag zu stellen. Dieses System validert schon ein wenig, versucht einen schon bestehenden Kunden im Partner-Systeme zu finden und schmeißt im Anschluss dieses Business-Event.\n",
    " * **AB-Test:** das Team Antrag ist stehst bemüht die User-Experience zu optimieren. So will es zum Beispiel die Durchlaufzeit auf ihrem eigenen System zu verkürzen. So variert es z.B. die Farbe des Knopfes. Wenn es bei einem Antrag von der Standardfarbe des Knopfes abweicht, schreibt es dieses technische Event\n",
    " * **Kunde hat Angebot aktzeptiert:** Nch\n",
    " * **Kunde hat Angebot abgelehnt:**\n",
    " \n",
    "#### Team Vertrag\n",
    " * **Antrag abgelehnt**: Ein Vertrag kann vom Vertragssystem abgelehnt werden. Dieses fragt z.B. noch bei Fraud an und validert auch sonst den Antrag viel \"besser\" als es das Antragssystem könnte.\n",
    " * **Kunde angelegt**: (eigentlich müsste ein anderes System dieses Event schreiben, aber in unserem Beispiel soll das mal OK sein) Wenn in einem Antrag nicht schon eine Referenz auf einen Kunden vorhanden ist, dann legt das Vertragssystem einen Kunden an und informiert Gott und die Welt hierüber.\n",
    " * **Vertrag angeboten**: Wenn ein Vertragsantrag aus Sicht des Vertragssystem gut aussieht, dann gibt es den Vertrag frei zur Annahme durch den Kunden. \n",
    " * **Vertrag policiert**: Wenn der Kunden ein Vertragsangebot akzeptiert hat, policieren wir den Vertrag. Nun ist der Kunde versichert. Jetzt können bedenkenlos Schadensfälle eintreten.\n",
    " \n",
    "#### Team Schaden\n",
    " * **Schaden reguliert**: wenn ein Schadensfall geprüft wurde und alles passt, dann überweisen wir Geld\n",
    "\n",
    "#### Übrigens\n",
    "**kunden.csv** ist nur ein Container mit allen Kunden ;-)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeeb418-98c1-436d-b13f-d319c4420962",
   "metadata": {},
   "source": [
    "## Erste Blicke in unsere Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcb483-6bc7-4786-9171-f5fad2a699fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunden_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"kunden.csv\")\n",
    "kunden_df.printSchema()\n",
    "kunden_df.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb40d3f-36c3-4e93-8b5a-0925cead5fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Soweit so bekannt. Wir könnten jetzt auf die Daten zugreifen in dem wir wie in den vorhergegangen Kapiteln filter anwenden, Null-Werte entfernen usw.\n",
    "Wir möchten aber absofort ganz normal mit SQL auf unsere Daten zugreifen. Dazu müssen wir zu einem Dataframe in Spark einen _View_ anlegen. Diese Views werden dann ganz normal wie Datenbanktabellen in SQL-Datenbanken behandelt (read-only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542e6b6-4131-4e40-8cc0-cc59eb30c4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunden_df.createOrReplaceTempView(\"kunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd663dc-7343-45ee-a11d-d4c239282abd",
   "metadata": {},
   "source": [
    "**Beachte:** die Funtion gibt nichts zurück. In der Spark-Application ist jetzt eine eine Tabelle unter dem Namen \"kunden\" verfügbar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29602ac9-7eec-46ca-8a78-d388b378bf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM kunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f3882-06ed-48bd-9a47-f15395c21d2c",
   "metadata": {},
   "source": [
    "Wir sehen das wir ein Dataframe zurückbekommen. Auf diesem können wir dann wieder die bekannten Operationen ausführen. Beachte, ob Du die Dateframe-API oder die SQL-API nutzt, hat in der Regel **keinen** Impact auf die Performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e8898-cb49-4cbc-a10d-6dd4515e8ea8",
   "metadata": {},
   "source": [
    "**Übung** setzte hier mal ein paar SQLs gegen diese Tabelle ab. Filter nach bestimmten Namen... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4036e-3276-4df3-b507-8ac26ae1bf94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM kunden').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879e92f-a73a-431f-a7ba-71ac0461eba6",
   "metadata": {},
   "source": [
    "## Alle Testdaten importieren\n",
    "und views anlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561abcf9-a58a-42e1-b2e3-857e7a7400f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AB-test\n",
    "ab_test_schema = \"AntragsId STRING\"\n",
    "ab_test_df = spark.read.option(\"header\", True).schema(ab_test_schema).csv(\"ab_test.csv\")\n",
    "ab_test_df.createOrReplaceTempView(\"ab_test\")\n",
    "\n",
    "# Antrag Abgelehnt\n",
    "antrag_abgelehnt_schema = \"AntragsId STRING, KundenId STRING, TimeStamp TIMESTAMP, Grund STRING\"\n",
    "antrag_abgelehnt_df = spark.read.option(\"header\", True).schema(antrag_abgelehnt_schema).csv(\"antrag_abgelehnt.csv\")\n",
    "antrag_abgelehnt_df.createOrReplaceTempView(\"antrag_abgelehnt\")\n",
    "\n",
    "# Antrag Erzeugt\n",
    "antrag_erzeugt_schema = \"AntragsId STRING, StartZeit TIMESTAMP, EndZeit TIMESTAMP, KundenId STRING\"\n",
    "antrag_erzeugt_df = spark.read.option(\"header\", True).schema(antrag_erzeugt_schema).csv(\"antrag_erzeugt.csv\")\n",
    "antrag_erzeugt_df.createOrReplaceTempView(\"antrag_erzeugt\")\n",
    "\n",
    "# Kunde Angelegt\n",
    "kunde_angelegt_schema = \"KundenId STRING, TimeStamp TIMESTAMP\"\n",
    "kunde_angelegt_df = spark.read.option(\"header\", True).schema(kunde_angelegt_schema).csv(\"kunde_angelegt.csv\")\n",
    "kunde_angelegt_df.createOrReplaceTempView(\"kunde_angelegt\")\n",
    "\n",
    "# Kunde hat Angebot Abgelehnt\n",
    "kunde_hat_angebot_abgelehnt_schema = \"VertragsId STRING, AntragsId STRING, Grund STRING, TimeStamp TIMESTAMP\"\n",
    "kunde_hat_angebot_abgelehnt_df = spark.read.option(\"header\", True).schema(kunde_hat_angebot_abgelehnt_schema).csv(\"kunde_hat_angebot_abgelehnt.csv\")\n",
    "kunde_hat_angebot_abgelehnt_df .createOrReplaceTempView(\"kunde_hat_angebot_abgelehnt\")\n",
    "\n",
    "# Kunde hat Angebot Akzeptiert\n",
    "kunde_hat_angebot_akzeptiert_schema = \"VertragsId STRING, AntragsId STRING, KundenId STRING, TimeStamp TIMESTAMP\"\n",
    "kunde_hat_angebot_akzeptiert_df = spark.read.option(\"header\", True).schema(kunde_hat_angebot_akzeptiert_schema).csv(\"kunde_hat_angebot_akzeptiert.csv\")\n",
    "kunde_hat_angebot_akzeptiert_df.createOrReplaceTempView(\"kunde_hat_angebot_akzeptiert\")\n",
    "\n",
    "# Schaden Gemeldet\n",
    "schaden_gemeldet_schema = \"VertagsId STRING, SchadensId STRING, Schadenshoehe INT, TimeStamp TIMESTAMP\"\n",
    "schaden_gemeldet_df = spark.read.option(\"header\", True).schema(schaden_gemeldet_schema).csv(\"schaden_gemeldet.csv\")\n",
    "schaden_gemeldet_df.createOrReplaceTempView(\"schaden_gemeldet\")\n",
    "\n",
    "# Schaden Reguliert\n",
    "schaden_reguliert_schema = \"SchadensId STRING, TimeStamp TIMESTAMP\"\n",
    "schaden_reguliert_df = spark.read.option(\"header\", True).schema(schaden_reguliert_schema).csv(\"schaden_reguliert.csv\")\n",
    "schaden_reguliert_df.createOrReplaceTempView(\"schaden_reguliert\")\n",
    "\n",
    "# Vertrag Angeboten\n",
    "vertrag_angeboten_schema = \"VertragsId STRING, AntragsId STRING, KundenId STRING, TimeStamp TIMESTAMP\"\n",
    "vertrag_angeboten_df = spark.read.option(\"header\", True).schema(vertrag_angeboten_schema).csv(\"vertrag_angeboten.csv\")\n",
    "vertrag_angeboten_df.createOrReplaceTempView(\"vertrag_angeboten\")\n",
    "\n",
    "# Vertrag Policiert\n",
    "vertrag_policiert_schema = \"VertragsId STRING, TimeStamp TIMESTAMP\"\n",
    "vertrag_policiert_df = spark.read.option(\"header\", True).schema(vertrag_policiert_schema).csv(\"vertrag_policiert.csv\")\n",
    "vertrag_policiert_df.createOrReplaceTempView(\"vertrag_policiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443150a9-ae9f-43a0-b299-f7f49f157123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alles da?\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50f92f-8c67-4938-a6d5-768ba374cf36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stichprobe bzgl. der Datentypen?\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED schaden_gemeldet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3661e5a-a183-41b1-92f1-50fc718b3618",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ein erstes Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5d6f0-0e58-40f4-9780-6d9aafee0ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zeige alle Anträge an, bei den wir die Farbe des Knopfs verändert haben\n",
    "\n",
    "# erstmal alle Anträge\n",
    "\n",
    "spark.sql(\"SELECT count(1) FROM antrag_erzeugt\").show()\n",
    "spark.sql(\"\"\"\n",
    "     SELECT count(1)\n",
    "       FROM antrag_erzeugt\n",
    " INNER JOIN ab_test\n",
    "         ON antrag_erzeugt.AntragsId = ab_test.AntragsId\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58183d96-c895-41ea-9b36-ea8e71776a0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Tables\n",
    "\n",
    "Bisher haben wir gesehen wie wir CSV Dateien laden und speichern können.\n",
    "Dabei mussten wir allerdings z.B. immer ein Schema angeben.\n",
    "Das ist auf Dauer lästig und mindestens aufwendig.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f1a09-5220-4896-b3d0-67c5c8e489e0",
   "metadata": {},
   "source": [
    "![](spark-catalog.dio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cede6-1c4a-4bb2-b6d9-913cd1d11686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# beispiel mit dem Kunde angelegt Event\n",
    "kunde_angelegt_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cecf6-5b8c-4750-83e4-803566d0cb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunde_angelegt_df.write.mode(\"overwrite\").saveAsTable(\"myTestTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc471a-bb43-4883-93a0-dd622203cea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunde_angelegt_df = spark.read.table(\"myTestTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235f26c-1453-47da-aff6-2c770b53d58d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunde_angelegt_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c52561-1dfc-4137-9025-9e1d1c74d5a9",
   "metadata": {},
   "source": [
    "wir können aber auch direkt mit SQL aus einer Table lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa42a73-c842-40f1-8483-a7860c02309e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM myTestTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f002581-b254-4ed3-a840-03894cd696d5",
   "metadata": {},
   "source": [
    "### Typen von Catalogs\n",
    "\n",
    "\n",
    "* **In-memory Catalog** Wenn eine Spark-Sitzung ended, wird auch der Catalog aufgeräumt\n",
    "* **Persistent Catalog** Metadaten werden permanent gespeichert, Spark hat einen [Hive](https://hive.apache.org/)-basierten Catalog eingebaut.\n",
    "\n",
    "### Typen von Tables\n",
    "\n",
    "* **Managed Tables** Spark kümmert sich um den Lebenszyklus. Es werden Daten und Schemata verwaltet. Nützlich für staging Daten. Wenn eine Tabelle gelöscht wird, dann werden Daten und Schemata gelöscht.\n",
    "* **Unmanged Tables/External Tables** Das Schema wird auch hier von Spark gemanaged, aber die Daten in einer frei wählbaren location. Löschen einer Tabelle löscht hier nur das Schema. Nützlich für das Ergebniss von ETL Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f02ca-6964-4f02-b50b-21cd3c42096b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from IPython.display import *\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"SparkTablesApp\")\n",
    "        .master(\"local[4]\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "        \n",
    "        .enableHiveSupport() # <---------------------------------\n",
    "    \n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc15e8-4dcf-42e8-8c52-d92d451cb47f",
   "metadata": {},
   "source": [
    "### Nun eine Datenbank in Hive erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6875fe-33f8-48f9-9601-51053c608936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SHOW DATABASES\n",
    "  \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099fd9f-19de-4189-9ac4-5d8b33f6507f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS Foo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f7789-bea6-4227-a7bc-ba3dd40d5fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW DATABASES\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648f4fb-3f85-47ab-8551-6f7cdc4c752e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunde_angelegt_df.write.mode(\"overwrite\").saveAsTable(\"foo.bar\")\n",
    "# wurde als managed table abgespeichert, da keine externe location angegeben wurde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacfcd3-f208-4335-9fa8-b97e5f7985c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW TABLES in foo\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6729ea1-241c-4ece-b93a-05fc0fbf52de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * from foo.bar\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d654b9-2271-4d8d-988e-d07529f63c93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# oder auch die tablle direkt in ein Dataframe lesen\n",
    "\n",
    "tmp = spark.read.table(\"foo.bar\")\n",
    "tmp.printSchema()\n",
    "tmp.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6fa565-9e03-4567-97e4-938bb261ba2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nun schauen wir uns mal die details der Tabelle an\n",
    "spark.sql(\"\"\"\n",
    "    DESCRIBE TABLE EXTENDED foo.bar\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82775ae5-c77b-4ede-af98-b194fcccd001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nun eine UNMANAGED Table anlegen\n",
    "(\n",
    "    kunden_df\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"path\", \"/home/pupil/spark-course/course/03-SQL/spark-warehouse/persistent/kunden.parquet\")\n",
    "        #.option(\"format\", \"csv\") # defaults to parquet\n",
    "        .saveAsTable(\"foo.bar\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3370f8c-7f02-4b72-ad18-eb7d1a8121e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    " DESCRIBE TABLE EXTENDED foo.bar\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec1b04e-79d5-4ef7-8838-fb0a40df3d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "DROP TABLE foo.bar\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55aee7-4840-4f3e-9f90-a42124fa4309",
   "metadata": {},
   "source": [
    "im datei browser anschauen, dass die Datein nicht wirklich gelöscht wurden\n",
    "Nun mit SQL die Tabelle wieder herstllen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d0136-bc5f-4242-937e-175d94b19940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nun eine Table direkt aus parquet herstellen (parquet hat Schema eingebaut ;-) )\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE  foo.bar\n",
    "    USING PARQUET\n",
    "    LOCATION \"/home/pupil/spark-course/course/03-SQL/spark-warehouse/persistent/kunden.parquet\"\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74caf641-a5a0-492c-a31b-b5b7b8625f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM foo.bar\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda0b87-78e4-4910-ae5e-d1e9c02cc3e7",
   "metadata": {},
   "source": [
    "**Take Away** Spark Tables machen die Entwicklung schneller und einfacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a32950-e0eb-402c-b782-0465657e02e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514cd5f-36fa-4f5d-9d6f-295970d6690e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a481ed8-cdbc-4e42-b79c-8caec2b5ff92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c707ff-9466-41f0-b7c5-edeeafbac132",
   "metadata": {},
   "source": [
    "## Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c97a9-dd93-431f-bb61-af38948dfbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": "<mxfile host=\"pupil-a.bin-ich-tot.de\" modified=\"2023-09-04T09:35:05.741Z\" agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\" version=\"21.1.6\" etag=\"V5d1q9Xg-VIdT2s2M7ay\" type=\"embed\">\n  <diagram id=\"zsaFwEb8lurTMNKA9gXb\" name=\"Page-1\">\n    <mxGraphModel dx=\"1036\" dy=\"570\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"827\" pageHeight=\"1169\" math=\"0\" shadow=\"0\">\n      <root>\n        <mxCell id=\"0\" />\n        <mxCell id=\"1\" parent=\"0\" />\n        <mxCell id=\"2\" value=\"\" style=\"rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"120\" y=\"120\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"3\" value=\"\" style=\"rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"294\" y=\"130\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n      </root>\n    </mxGraphModel>\n  </diagram>\n</mxfile>\n"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
