{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c9d657-e97c-45b9-b132-4f9a1e817c09",
   "metadata": {},
   "source": [
    "# Der Höhepunkt: SQL, Spark-Tables, Reporting ...\n",
    "\n",
    "* SQL Abfragen auf Dataframes anwenden\n",
    "* Mit Spark Datenbanken & Tabellen arbeiten\n",
    "* User-Defined Functions benutzen\n",
    "* Datenmegen joinen\n",
    "* Window Operationen\n",
    "* Testdaten erzeugen, welche zwei Geschäftsprozesse aus der Versicherungswelt abbilden\n",
    "* Die ersten (und wesentlichsten) Schritte für ein sinnvolles Monitoring dieser Prozesse gehen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb03304-9de0-40e1-b3ac-843e16c0ef9c",
   "metadata": {},
   "source": [
    "## Wie immer eine Spark-Application registieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b904ba-baea-447b-86a5-0640e887ee74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/lib/python3.10/dist-packages/pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28fd1b01-e8cb-4f68-b89c-4ecce0c5d0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 11:07:01 WARN Utils: Your hostname, pupil-a resolves to a loopback address: 127.0.1.1; using 23.88.105.62 instead (on interface eth0)\n",
      "23/09/04 11:07:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/04 11:07:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pupil-a.bin-ich-tot.de:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sql</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6a7fe272e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"sql\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\",False)\n",
    "        .config(\"spark.sql.adaptive.enabled\",False)\n",
    "        .master(\"local[4]\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb782484-df8a-4887-85b3-ff1a071b3f85",
   "metadata": {},
   "source": [
    "## Nun Testdaten erzeugen\n",
    "Hierzu haben wir ein Beispiel aus der Versicherungswelt vorbereiten. Vielleicht ist es dem in der Signal sogar ein bisschen ähnlich ;-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe6873-4e84-43c4-9ebe-80ada0d84831",
   "metadata": {},
   "source": [
    "![](architecture-level-01.dio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e73784-9780-4b5c-a47b-30e6cdfdf20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import generate_test_data\n",
    "generate_test_data.main(1000) # führe das nur _einmal_ aus!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467f314-57c1-498e-9a7f-e91e5350e433",
   "metadata": {
    "tags": []
   },
   "source": [
    "Wir sehen nun auf im Dateibrowser eine Liste von Dateien, die überwiegend fachliche Events enthalten, die zwei Geschäftsprozesse abbilden:\n",
    "\n",
    "### Ein Kunden bekommt einen neuen Vertrag\n",
    "\n",
    "Die Systeme welche die Events schreiben liegen in der Verantwortung verschiedener Teams:\n",
    "\n",
    "#### Team Antrag\n",
    " * **Antrag Erzeugt:** Der Kunde geht im Internet auf ein System um einen Antrag auf einen Versicherungsvertrag zu stellen. Dieses System validert schon ein wenig, versucht einen schon bestehenden Kunden im Partner-Systeme zu finden und schmeißt im Anschluss dieses Business-Event.\n",
    " * **AB-Test:** das Team Antrag ist stehst bemüht die User-Experience zu optimieren. So will es zum Beispiel die Durchlaufzeit auf ihrem eigenen System zu verkürzen. So variert es z.B. die Farbe des Knopfes. Wenn es bei einem Antrag von der Standardfarbe des Knopfes abweicht, schreibt es dieses technische Event\n",
    " * **Kunde hat Angebot aktzeptiert:** Nachdem das Vertragssystem sein grünes Licht gegeben hat, wurde der Vertrag vom Kunden auch akzeptiert.\n",
    " * **Kunde hat Angebot abgelehnt:** Wie vorhergehendes Event nur der Kunde will das Angebot nicht annehmen.\n",
    " \n",
    "#### Team Vertrag\n",
    " * **Antrag abgelehnt**: Ein Vertrag kann vom Vertragssystem abgelehnt werden. Dieses fragt z.B. noch bei Fraud an und validert auch sonst den Antrag viel \"besser\" als es das Antragssystem könnte.\n",
    " * **Kunde angelegt**: (eigentlich müsste ein anderes System dieses Event schreiben, aber in unserem Beispiel soll das mal OK sein) Wenn in einem Antrag nicht schon eine Referenz auf einen Kunden vorhanden ist, dann legt das Vertragssystem einen Kunden an und informiert Gott und die Welt hierüber.\n",
    " * **Vertrag angeboten**: Wenn ein Vertragsantrag aus Sicht des Vertragssystem gut aussieht, dann gibt es den Vertrag frei zur Annahme durch den Kunden. \n",
    " * **Vertrag policiert**: Wenn der Kunden ein Vertragsangebot akzeptiert hat, policieren wir den Vertrag. Nun ist der Kunde versichert. Jetzt können bedenkenlos Schadensfälle eintreten.\n",
    " \n",
    "#### Team Schaden\n",
    " * **Schaden reguliert**: wenn ein Schadensfall geprüft wurde und alles passt, dann überweisen wir Geld\n",
    "\n",
    "#### Übrigens\n",
    "**kunden.csv** ist nur ein Container mit allen Kunden ;-)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeeb418-98c1-436d-b13f-d319c4420962",
   "metadata": {},
   "source": [
    "## Erste Blicke in unsere Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bcb483-6bc7-4786-9171-f5fad2a699fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------------------------------------+-----------------+\n",
      "|id                                  |name             |\n",
      "+------------------------------------+-----------------+\n",
      "|ce4cf720-4a1a-11ee-8349-7b31dfcc7252|gorzala          |\n",
      "|d55a2ede-4a1a-11ee-8cce-b739d8c6e256|Scharrenberch    |\n",
      "|793401ed-7462-4201-9846-eaf540936914|Gracious Jackson |\n",
      "|ecc213f5-96cd-42c4-a018-d60d98064803|Mystifying Edison|\n",
      "+------------------------------------+-----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kunden_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"kunden.csv\")\n",
    "kunden_df.printSchema()\n",
    "kunden_df.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb40d3f-36c3-4e93-8b5a-0925cead5fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Soweit so bekannt. Wir könnten jetzt auf die Daten zugreifen in dem wir wie in den vorhergegangen Kapiteln filter anwenden, Null-Werte entfernen usw.\n",
    "Wir möchten aber absofort ganz normal mit SQL auf unsere Daten zugreifen. Dazu müssen wir zu einem Dataframe in Spark einen _View_ anlegen. Diese Views werden dann ganz normal wie Datenbanktabellen in SQL-Datenbanken behandelt (read-only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5542e6b6-4131-4e40-8cc0-cc59eb30c4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunden_df.createOrReplaceTempView(\"kunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd663dc-7343-45ee-a11d-d4c239282abd",
   "metadata": {},
   "source": [
    "**Beachte:** die Funtion gibt nichts zurück. In der Spark-Application ist jetzt eine eine Tabelle unter dem Namen \"kunden\" verfügbar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29602ac9-7eec-46ca-8a78-d388b378bf57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, name: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM kunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f3882-06ed-48bd-9a47-f15395c21d2c",
   "metadata": {},
   "source": [
    "Wir sehen das wir ein Dataframe zurückbekommen. Auf diesem können wir dann wieder die bekannten Operationen ausführen. Beachte, ob Du die Dateframe-API oder die SQL-API nutzt, hat in der Regel **keinen** Impact auf die Performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e8898-cb49-4cbc-a10d-6dd4515e8ea8",
   "metadata": {},
   "source": [
    "**Übung** setzte hier mal ein paar SQLs gegen diese Tabelle ab. Filter nach bestimmten Namen... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df4036e-3276-4df3-b507-8ac26ae1bf94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|                  id|             name|\n",
      "+--------------------+-----------------+\n",
      "|ce4cf720-4a1a-11e...|          gorzala|\n",
      "|d55a2ede-4a1a-11e...|    Scharrenberch|\n",
      "|793401ed-7462-420...| Gracious Jackson|\n",
      "|ecc213f5-96cd-42c...|Mystifying Edison|\n",
      "|abbcb555-a501-433...| Goofy Hofstadter|\n",
      "|0cef6d61-b4a2-48e...|  Relaxed Meitner|\n",
      "|a5b96d1f-8f2a-487...|    Gallant Moore|\n",
      "|18010b35-daff-42b...|   Tender Noether|\n",
      "|8e6324df-094a-44c...|    Tender Galois|\n",
      "|1d247f77-251e-425...|    Crazy Poitras|\n",
      "|bb8b1ec7-a7b4-49b...|Magical Ramanujan|\n",
      "|e9fbd5b7-8da2-4aa...|     Frosty Tharp|\n",
      "|09aa0bae-1e51-40a...|  Festive Hellman|\n",
      "|016e722d-64a6-494...|        Epic Cori|\n",
      "|8555b696-5cda-4f3...|    Great Hellman|\n",
      "|e2fb710f-2036-497...|Xenodochial Gould|\n",
      "|784161e0-9e1a-40c...|Suspicious Turing|\n",
      "|6ccab604-aa98-421...|    Happy Hellman|\n",
      "|07bb7d22-ebf6-476...|    Reverent Pike|\n",
      "|e11ee24e-c219-4d1...|    Infallible Wu|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM kunden').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879e92f-a73a-431f-a7ba-71ac0461eba6",
   "metadata": {},
   "source": [
    "## Alle Testdaten importieren\n",
    "und views anlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561abcf9-a58a-42e1-b2e3-857e7a7400f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AB-test\n",
    "ab_test_schema = \"AntragsId STRING\"\n",
    "ab_test_df = spark.read.option(\"header\", True).schema(ab_test_schema).csv(\"ab_test.csv\")\n",
    "ab_test_df.createOrReplaceTempView(\"ab_test\")\n",
    "\n",
    "# Antrag Abgelehnt\n",
    "antrag_abgelehnt_schema = \"AntragsId STRING, KundenId STRING, TimeStamp TIMESTAMP, Grund STRING\"\n",
    "antrag_abgelehnt_df = spark.read.option(\"header\", True).schema(antrag_abgelehnt_schema).csv(\"antrag_abgelehnt.csv\")\n",
    "antrag_abgelehnt_df.createOrReplaceTempView(\"antrag_abgelehnt\")\n",
    "\n",
    "# Antrag Erzeugt\n",
    "antrag_erzeugt_schema = \"AntragsId STRING, StartZeit TIMESTAMP, EndZeit TIMESTAMP, KundenId STRING\"\n",
    "antrag_erzeugt_df = spark.read.option(\"header\", True).schema(antrag_erzeugt_schema).csv(\"antrag_erzeugt.csv\")\n",
    "antrag_erzeugt_df.createOrReplaceTempView(\"antrag_erzeugt\")\n",
    "\n",
    "# Kunde Angelegt\n",
    "kunde_angelegt_schema = \"KundenId STRING, TimeStamp TIMESTAMP\"\n",
    "kunde_angelegt_df = spark.read.option(\"header\", True).schema(kunde_angelegt_schema).csv(\"kunde_angelegt.csv\")\n",
    "kunde_angelegt_df.createOrReplaceTempView(\"kunde_angelegt\")\n",
    "\n",
    "# Kunde hat Angebot Abgelehnt\n",
    "kunde_hat_angebot_abgelehnt_schema = \"VertragsId STRING, AntragsId STRING, Grund STRING, TimeStamp TIMESTAMP\"\n",
    "kunde_hat_angebot_abgelehnt_df = spark.read.option(\"header\", True).schema(kunde_hat_angebot_abgelehnt_schema).csv(\"kunde_hat_angebot_abgelehnt.csv\")\n",
    "kunde_hat_angebot_abgelehnt_df .createOrReplaceTempView(\"kunde_hat_angebot_abgelehnt\")\n",
    "\n",
    "# Kunde hat Angebot Akzeptiert\n",
    "kunde_hat_angebot_akzeptiert_schema = \"VertragsId STRING, AntragsId STRING, KundenId STRING, TimeStamp TIMESTAMP\"\n",
    "kunde_hat_angebot_akzeptiert_df = spark.read.option(\"header\", True).schema(kunde_hat_angebot_akzeptiert_schema).csv(\"kunde_hat_angebot_akzeptiert.csv\")\n",
    "kunde_hat_angebot_akzeptiert_df.createOrReplaceTempView(\"kunde_hat_angebot_akzeptiert\")\n",
    "\n",
    "# Schaden Gemeldet\n",
    "schaden_gemeldet_schema = \"VertagsId STRING, SchadensId STRING, Schadenshoehe INT, TimeStamp TIMESTAMP\"\n",
    "schaden_gemeldet_df = spark.read.option(\"header\", True).schema(schaden_gemeldet_schema).csv(\"schaden_gemeldet.csv\")\n",
    "schaden_gemeldet_df.createOrReplaceTempView(\"schaden_gemeldet\")\n",
    "\n",
    "# Schaden Reguliert\n",
    "schaden_reguliert_schema = \"SchadensId STRING, TimeStamp TIMESTAMP\"\n",
    "schaden_reguliert_df = spark.read.option(\"header\", True).schema(schaden_reguliert_schema).csv(\"schaden_reguliert.csv\")\n",
    "schaden_reguliert_df.createOrReplaceTempView(\"schaden_reguliert\")\n",
    "\n",
    "# Vertrag Angeboten\n",
    "vertrag_angeboten_schema = \"VertragsId STRING, AntragsId STRING, KundenId STRING, TimeStamp TIMESTAMP\"\n",
    "vertrag_angeboten_df = spark.read.option(\"header\", True).schema(vertrag_angeboten_schema).csv(\"vertrag_angeboten.csv\")\n",
    "vertrag_angeboten_df.createOrReplaceTempView(\"vertrag_angeboten\")\n",
    "\n",
    "# Vertrag Policiert\n",
    "vertrag_policiert_schema = \"VertragsId STRING, TimeStamp TIMESTAMP\"\n",
    "vertrag_policiert_df = spark.read.option(\"header\", True).schema(vertrag_policiert_schema).csv(\"vertrag_policiert.csv\")\n",
    "vertrag_policiert_df.createOrReplaceTempView(\"vertrag_policiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "443150a9-ae9f-43a0-b299-f7f49f157123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------+-----------+\n",
      "|namespace|tableName                   |isTemporary|\n",
      "+---------+----------------------------+-----------+\n",
      "|         |ab_test                     |true       |\n",
      "|         |antrag_abgelehnt            |true       |\n",
      "|         |antrag_erzeugt              |true       |\n",
      "|         |kunde_angelegt              |true       |\n",
      "|         |kunde_hat_angebot_abgelehnt |true       |\n",
      "|         |kunde_hat_angebot_akzeptiert|true       |\n",
      "|         |kunden                      |true       |\n",
      "|         |schaden_gemeldet            |true       |\n",
      "|         |schaden_reguliert           |true       |\n",
      "|         |vertrag_angeboten           |true       |\n",
      "|         |vertrag_policiert           |true       |\n",
      "+---------+----------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alles da?\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc50f92f-8c67-4938-a6d5-768ba374cf36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|    VertagsId|   string|   null|\n",
      "|   SchadensId|   string|   null|\n",
      "|Schadenshoehe|      int|   null|\n",
      "|    TimeStamp|timestamp|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stichprobe bzgl. der Datentypen?\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED schaden_gemeldet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3661e5a-a183-41b1-92f1-50fc718b3618",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ein erstes Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2e5d6f0-0e58-40f4-9780-6d9aafee0ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     105|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zeige alle Anträge an, bei den wir die Farbe des Knopfs verändert haben\n",
    "\n",
    "# erstmal alle Anträge\n",
    "\n",
    "spark.sql(\"SELECT count(1) FROM antrag_erzeugt\").show()\n",
    "spark.sql(\"\"\"\n",
    "     SELECT count(1)\n",
    "       FROM antrag_erzeugt\n",
    " INNER JOIN ab_test\n",
    "         ON antrag_erzeugt.AntragsId = ab_test.AntragsId\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58183d96-c895-41ea-9b36-ea8e71776a0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Tables\n",
    "\n",
    "Bisher haben wir gesehen wie wir CSV Dateien laden und speichern können.\n",
    "Dabei mussten wir allerdings z.B. immer ein Schema angeben.\n",
    "Das ist auf Dauer lästig und mindestens aufwendig.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f1a09-5220-4896-b3d0-67c5c8e489e0",
   "metadata": {},
   "source": [
    "![](spark-catalog.dio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5cede6-1c4a-4bb2-b6d9-913cd1d11686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- KundenId: string (nullable = true)\n",
      " |-- TimeStamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# beispiel mit dem Kunde angelegt Event\n",
    "kunde_angelegt_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968cecf6-5b8c-4750-83e4-803566d0cb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kunde_angelegt_df.write.mode(\"overwrite\").saveAsTable(\"myTestTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "effc471a-bb43-4883-93a0-dd622203cea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunde_angelegt_df = spark.read.table(\"myTestTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3235f26c-1453-47da-aff6-2c770b53d58d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- KundenId: string (nullable = true)\n",
      " |-- TimeStamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kunde_angelegt_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c52561-1dfc-4137-9025-9e1d1c74d5a9",
   "metadata": {},
   "source": [
    "wir können aber auch direkt mit SQL aus einer Table lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aa42a73-c842-40f1-8483-a7860c02309e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[KundenId: string, TimeStamp: timestamp]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM myTestTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f002581-b254-4ed3-a840-03894cd696d5",
   "metadata": {},
   "source": [
    "### Typen von Catalogs\n",
    "\n",
    "\n",
    "* **In-memory Catalog** Wenn eine Spark-Sitzung ended, wird auch der Catalog aufgeräumt\n",
    "* **Persistent Catalog** Metadaten werden permanent gespeichert, Spark hat einen [Hive](https://hive.apache.org/)-basierten Catalog eingebaut.\n",
    "\n",
    "### Typen von Tables\n",
    "\n",
    "* **Managed Tables** Spark kümmert sich um den Lebenszyklus. Es werden Daten und Schemata verwaltet. Nützlich für staging Daten. Wenn eine Tabelle gelöscht wird, dann werden Daten und Schemata gelöscht.\n",
    "* **Unmanged Tables/External Tables** Das Schema wird auch hier von Spark gemanaged, aber die Daten in einer frei wählbaren location. Löschen einer Tabelle löscht hier nur das Schema. Nützlich für das Ergebniss von ETL Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd8f02ca-6964-4f02-b50b-21cd3c42096b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:08:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pupil-a.bin-ich-tot.de:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sql</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6a7fe272e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from IPython.display import *\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"SparkTablesApp\")\n",
    "        .master(\"local[4]\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "        \n",
    "        .enableHiveSupport() # <---------------------------------\n",
    "    \n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc15e8-4dcf-42e8-8c52-d92d451cb47f",
   "metadata": {},
   "source": [
    "### Nun eine Datenbank in Hive erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a6875fe-33f8-48f9-9601-51053c608936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SHOW DATABASES\n",
    "  \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c099fd9f-19de-4189-9ac4-5d8b33f6507f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS Foo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b80f7789-bea6-4227-a7bc-ba3dd40d5fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|      foo|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW DATABASES\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4648f4fb-3f85-47ab-8551-6f7cdc4c752e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kunde_angelegt_df.write.mode(\"overwrite\").saveAsTable(\"foo.bar\")\n",
    "# wurde als managed table abgespeichert, da keine externe location angegeben wurde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdacfcd3-f208-4335-9fa8-b97e5f7985c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------+-----------+\n",
      "|namespace|tableName                   |isTemporary|\n",
      "+---------+----------------------------+-----------+\n",
      "|foo      |bar                         |false      |\n",
      "|         |ab_test                     |true       |\n",
      "|         |antrag_abgelehnt            |true       |\n",
      "|         |antrag_erzeugt              |true       |\n",
      "|         |kunde_angelegt              |true       |\n",
      "|         |kunde_hat_angebot_abgelehnt |true       |\n",
      "|         |kunde_hat_angebot_akzeptiert|true       |\n",
      "|         |kunden                      |true       |\n",
      "|         |schaden_gemeldet            |true       |\n",
      "|         |schaden_reguliert           |true       |\n",
      "|         |vertrag_angeboten           |true       |\n",
      "|         |vertrag_policiert           |true       |\n",
      "+---------+----------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW TABLES in foo\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6729ea1-241c-4ece-b93a-05fc0fbf52de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            KundenId|          TimeStamp|\n",
      "+--------------------+-------------------+\n",
      "|ecc213f5-96cd-42c...|2023-09-10 12:35:52|\n",
      "|0cef6d61-b4a2-48e...|2023-09-07 09:11:09|\n",
      "|784161e0-9e1a-40c...|2023-09-11 07:18:43|\n",
      "|07bb7d22-ebf6-476...|2023-09-06 11:38:30|\n",
      "|e11ee24e-c219-4d1...|2023-09-05 09:30:03|\n",
      "|50a9152a-5ee0-41c...|2023-09-07 21:43:58|\n",
      "|0fbb6c1e-89a1-402...|2023-09-09 11:11:13|\n",
      "|f7029b08-1af0-44f...|2023-09-06 10:26:25|\n",
      "|cdf5f5db-6fd0-4ed...|2023-09-20 09:39:13|\n",
      "|d4d3cb7d-258f-401...|2023-09-17 04:33:27|\n",
      "|70efc788-9d7b-4df...|2023-09-09 09:23:01|\n",
      "|07a8f7bd-6f25-4c4...|2023-09-10 06:29:38|\n",
      "|e63e36a2-fdb7-4bd...|2023-09-17 11:56:09|\n",
      "|33e5a05c-680e-4aa...|2023-09-03 05:43:06|\n",
      "|afdc1487-286f-4be...|2023-09-06 09:21:01|\n",
      "|3667b3ed-4339-426...|2023-09-11 13:25:35|\n",
      "|aa5ea83d-7860-467...|2023-09-07 04:29:21|\n",
      "|341152a4-e731-4b8...|2023-09-06 23:18:30|\n",
      "|e998f425-8f52-4ef...|2023-09-13 08:31:26|\n",
      "|5a96afb2-4f0d-4c3...|2023-09-06 00:28:19|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * from foo.bar\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35d654b9-2271-4d8d-988e-d07529f63c93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- KundenId: string (nullable = true)\n",
      " |-- TimeStamp: timestamp (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+\n",
      "|            KundenId|          TimeStamp|\n",
      "+--------------------+-------------------+\n",
      "|ecc213f5-96cd-42c...|2023-09-10 12:35:52|\n",
      "|0cef6d61-b4a2-48e...|2023-09-07 09:11:09|\n",
      "+--------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# oder auch die tablle direkt in ein Dataframe lesen\n",
    "\n",
    "tmp = spark.read.table(\"foo.bar\")\n",
    "tmp.printSchema()\n",
    "tmp.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a6fa565-9e03-4567-97e4-938bb261ba2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                             |comment|\n",
      "+----------------------------+----------------------------------------------------------------------+-------+\n",
      "|KundenId                    |string                                                                |null   |\n",
      "|TimeStamp                   |timestamp                                                             |null   |\n",
      "|                            |                                                                      |       |\n",
      "|# Detailed Table Information|                                                                      |       |\n",
      "|Catalog                     |spark_catalog                                                         |       |\n",
      "|Database                    |foo                                                                   |       |\n",
      "|Table                       |bar                                                                   |       |\n",
      "|Created Time                |Mon Sep 04 12:08:14 UTC 2023                                          |       |\n",
      "|Last Access                 |UNKNOWN                                                               |       |\n",
      "|Created By                  |Spark 3.4.1                                                           |       |\n",
      "|Type                        |MANAGED                                                               |       |\n",
      "|Provider                    |parquet                                                               |       |\n",
      "|Location                    |file:/home/pupil/spark-course/course/03-SQL/spark-warehouse/foo.db/bar|       |\n",
      "+----------------------------+----------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nun schauen wir uns mal die details der Tabelle an\n",
    "spark.sql(\"\"\"\n",
    "    DESCRIBE TABLE EXTENDED foo.bar\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82775ae5-c77b-4ede-af98-b194fcccd001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nun eine UNMANAGED Table anlegen\n",
    "(\n",
    "    kunden_df\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"path\", \"/home/pupil/spark-course/course/03-SQL/spark-warehouse/persistent/kunden.parquet\")\n",
    "        #.option(\"format\", \"csv\") # defaults to parquet\n",
    "        .saveAsTable(\"foo.bar\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3370f8c-7f02-4b72-ad18-eb7d1a8121e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                              |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------------------+-------+\n",
      "|id                          |string                                                                                 |null   |\n",
      "|name                        |string                                                                                 |null   |\n",
      "|                            |                                                                                       |       |\n",
      "|# Detailed Table Information|                                                                                       |       |\n",
      "|Catalog                     |spark_catalog                                                                          |       |\n",
      "|Database                    |foo                                                                                    |       |\n",
      "|Table                       |bar                                                                                    |       |\n",
      "|Created Time                |Mon Sep 04 12:08:15 UTC 2023                                                           |       |\n",
      "|Last Access                 |UNKNOWN                                                                                |       |\n",
      "|Created By                  |Spark 3.4.1                                                                            |       |\n",
      "|Type                        |EXTERNAL                                                                               |       |\n",
      "|Provider                    |parquet                                                                                |       |\n",
      "|Location                    |file:///home/pupil/spark-course/course/03-SQL/spark-warehouse/persistent/kunden.parquet|       |\n",
      "+----------------------------+---------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    " DESCRIBE TABLE EXTENDED foo.bar\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ec1b04e-79d5-4ef7-8838-fb0a40df3d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DROP TABLE foo.bar\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55aee7-4840-4f3e-9f90-a42124fa4309",
   "metadata": {},
   "source": [
    "im datei browser anschauen, dass die Datein nicht wirklich gelöscht wurden\n",
    "Nun mit SQL die Tabelle wieder herstllen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "019d0136-bc5f-4242-937e-175d94b19940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nun eine Table direkt aus parquet herstellen (parquet hat Schema eingebaut ;-) )\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE  foo.bar\n",
    "    USING PARQUET\n",
    "    LOCATION \"/home/pupil/spark-course/course/03-SQL/spark-warehouse/persistent/kunden.parquet\"\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74caf641-a5a0-492c-a31b-b5b7b8625f64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|                  id|             name|\n",
      "+--------------------+-----------------+\n",
      "|ce4cf720-4a1a-11e...|          gorzala|\n",
      "|d55a2ede-4a1a-11e...|    Scharrenberch|\n",
      "|793401ed-7462-420...| Gracious Jackson|\n",
      "|ecc213f5-96cd-42c...|Mystifying Edison|\n",
      "|abbcb555-a501-433...| Goofy Hofstadter|\n",
      "|0cef6d61-b4a2-48e...|  Relaxed Meitner|\n",
      "|a5b96d1f-8f2a-487...|    Gallant Moore|\n",
      "|18010b35-daff-42b...|   Tender Noether|\n",
      "|8e6324df-094a-44c...|    Tender Galois|\n",
      "|1d247f77-251e-425...|    Crazy Poitras|\n",
      "|bb8b1ec7-a7b4-49b...|Magical Ramanujan|\n",
      "|e9fbd5b7-8da2-4aa...|     Frosty Tharp|\n",
      "|09aa0bae-1e51-40a...|  Festive Hellman|\n",
      "|016e722d-64a6-494...|        Epic Cori|\n",
      "|8555b696-5cda-4f3...|    Great Hellman|\n",
      "|e2fb710f-2036-497...|Xenodochial Gould|\n",
      "|784161e0-9e1a-40c...|Suspicious Turing|\n",
      "|6ccab604-aa98-421...|    Happy Hellman|\n",
      "|07bb7d22-ebf6-476...|    Reverent Pike|\n",
      "|e11ee24e-c219-4d1...|    Infallible Wu|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM foo.bar\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda0b87-78e4-4910-ae5e-d1e9c02cc3e7",
   "metadata": {},
   "source": [
    "**Take Away** Spark Tables machen die Entwicklung schneller und einfacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a32950-e0eb-402c-b782-0465657e02e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark User Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43b6a6-0dd4-45bc-9dcf-5ee40e2bc7f0",
   "metadata": {},
   "source": [
    "Spark stellt dir zwar eine Menge an Funktionen zur Datenmanipulation zur Verfügung aber machnchmal fehlt doch etwas.\n",
    "Aus diesem Grund gibt es User Defined Funktions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc0023f0-db16-48c3-97fa-02cad3b1a69d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-------------------+-------------------+------------------------------------+\n",
      "|AntragsId                           |StartZeit          |EndZeit            |KundenId                            |\n",
      "+------------------------------------+-------------------+-------------------+------------------------------------+\n",
      "|4713b4b8-e077-4c7a-8e7d-7306786c9773|2023-09-06 20:48:00|2023-09-06 20:57:00|793401ed-7462-4201-9846-eaf540936914|\n",
      "|e0337bbc-8bc7-4ff8-8e86-e495c40dd469|2023-09-10 12:32:52|2023-09-10 12:34:52|null                                |\n",
      "|ee2dd754-84f2-4f5c-89da-26e1a5c6f41b|2023-09-03 17:11:44|2023-09-03 17:19:44|null                                |\n",
      "|bb3c489a-8358-4aca-b6cd-c016a40baa9b|2023-09-05 18:00:55|2023-09-05 18:07:55|abbcb555-a501-4333-a73e-4f147a61b65b|\n",
      "|657c4d3d-e2f6-4705-8f0a-53e4eb23b96e|2023-09-07 09:01:09|2023-09-07 09:10:09|null                                |\n",
      "|8620d538-b37f-4835-ad3c-8f887cb36ebd|2023-09-03 01:02:45|2023-09-03 01:04:45|a5b96d1f-8f2a-487b-92b9-029655c52fd9|\n",
      "|dee13ece-374d-4f5d-b3d1-b7d6306feb4b|2023-09-03 20:06:52|2023-09-03 20:08:52|18010b35-daff-42be-8dc1-912eaf266fca|\n",
      "|90a36d23-3db8-4c1c-9408-dbfb588cce94|2023-09-04 15:19:16|2023-09-04 15:24:16|8e6324df-094a-44c0-95db-814731936cb9|\n",
      "|ecbea295-ede1-4e9f-b3e3-e8281d73f4d8|2023-09-19 10:37:11|2023-09-19 10:38:11|1d247f77-251e-4256-8daa-dfafdadbb866|\n",
      "|712d3153-5bda-4c44-ab55-26e9c7c809ec|2023-09-16 11:00:43|2023-09-16 11:05:43|bb8b1ec7-a7b4-49bb-baa8-c20c42d2f9cb|\n",
      "|237f2567-1de8-4cf0-80cb-efea2e3ef2ae|2023-09-10 21:01:23|2023-09-10 21:04:23|e9fbd5b7-8da2-4aa1-ad3a-055bedc0a660|\n",
      "|8722b357-2289-44a1-bff8-0860baa64a12|2023-09-16 20:40:02|2023-09-16 20:44:02|09aa0bae-1e51-40ad-8cb4-6868529b7204|\n",
      "|be84a9f0-b7b9-4fc1-bb99-285ea793697b|2023-09-05 23:43:44|2023-09-05 23:48:44|016e722d-64a6-494d-a4e5-1742900058ce|\n",
      "|cfe73f96-21dc-4960-a8e5-6232da8cb6a5|2023-09-14 09:46:18|2023-09-14 09:51:18|8555b696-5cda-4f31-9f07-17200271519f|\n",
      "|ca927440-90d3-4e63-afab-09d1640a3fae|2023-09-16 23:30:38|2023-09-16 23:36:38|e2fb710f-2036-4971-b3b1-343a2c519594|\n",
      "|b670ca28-c129-45b0-8e2d-38178e3c77b5|2023-09-11 07:13:43|2023-09-11 07:17:43|null                                |\n",
      "|62cccdb4-6943-4bb2-85f1-285486992afc|2023-09-07 17:17:25|2023-09-07 17:22:25|6ccab604-aa98-4217-85a8-41456ea44a78|\n",
      "|1f7f683d-c781-4478-afbe-4e276c01ec36|2023-09-06 11:30:30|2023-09-06 11:37:30|null                                |\n",
      "|feb59624-e73f-4f83-92a8-f6811c3db4db|2023-09-05 09:25:03|2023-09-05 09:29:03|null                                |\n",
      "|933cd2fe-9fb0-40f1-a59c-2e4db4a7efb8|2023-09-10 20:37:05|2023-09-10 20:41:05|2c618a63-84cc-477e-87d4-c6a38bcd746e|\n",
      "+------------------------------------+-------------------+-------------------+------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SELECT * FROM antrag_erzeugt\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec0a36-144a-496d-95fa-810355730e40",
   "metadata": {
    "tags": []
   },
   "source": [
    "Wir möchten hier zum Beispiel die Zeitauer als eigene Spalte haben, die der Kunde auf der Antragsstrecke verbracht hat. Das würde zwar auch mit SQL gehen, aber soll hier als Beispiel dienen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d07856-e51c-4c3b-9dda-70709d1b7fe6",
   "metadata": {},
   "source": [
    "**Beachte**: Für Spark sind UDFs Blackboxen und Spark wendet deswegen keinerlei Optimierungen auf deren Anwendung an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8fd2baf5-e52e-4f12-906f-5060090b74a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# meine erste UDF\n",
    "def duration(first_ts, second_ts):\n",
    "    delta = second_ts - first_ts\n",
    "    return int(delta.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "158e6140-97de-4280-a1d0-426b02e142d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duration_udf = udf( lambda first, second: duration(first,second), IntegerType()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d04ff-e25a-4e4a-b7c0-0e8fcec8cb6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Anwenden in DF-Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1418acd0-5ea5-4478-b332-5ca1ac5a84fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-------------------+-------------------+------------------------------------+--------+\n",
      "|AntragsId                           |StartZeit          |EndZeit            |KundenId                            |Duration|\n",
      "+------------------------------------+-------------------+-------------------+------------------------------------+--------+\n",
      "|4713b4b8-e077-4c7a-8e7d-7306786c9773|2023-09-06 20:48:00|2023-09-06 20:57:00|793401ed-7462-4201-9846-eaf540936914|540     |\n",
      "|e0337bbc-8bc7-4ff8-8e86-e495c40dd469|2023-09-10 12:32:52|2023-09-10 12:34:52|null                                |120     |\n",
      "|ee2dd754-84f2-4f5c-89da-26e1a5c6f41b|2023-09-03 17:11:44|2023-09-03 17:19:44|null                                |480     |\n",
      "|bb3c489a-8358-4aca-b6cd-c016a40baa9b|2023-09-05 18:00:55|2023-09-05 18:07:55|abbcb555-a501-4333-a73e-4f147a61b65b|420     |\n",
      "|657c4d3d-e2f6-4705-8f0a-53e4eb23b96e|2023-09-07 09:01:09|2023-09-07 09:10:09|null                                |540     |\n",
      "+------------------------------------+-------------------+-------------------+------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "antrag_erzeugt_df.select(\n",
    "    \"*\",\n",
    "    duration_udf(col(\"StartZeit\"), col(\"EndZeit\")).alias(\"Duration\")\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712a61f-a17f-4771-b6c1-a1fc5f2b6b1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Anwenden in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "64f7283a-2b4c-4cdf-8d99-4c280ebb69c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:31:45 WARN SimpleFunctionRegistry: The function duration replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.duration(first_ts, second_ts)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# für spark sql registrieren\n",
    "spark.udf.register(\"duration\", duration, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "229602ce-b00f-4512-a747-64f9648009d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-------------------+-------------------+------------------------------------+---+\n",
      "|AntragsId                           |StartZeit          |EndZeit            |KundenId                            |Foo|\n",
      "+------------------------------------+-------------------+-------------------+------------------------------------+---+\n",
      "|4713b4b8-e077-4c7a-8e7d-7306786c9773|2023-09-06 20:48:00|2023-09-06 20:57:00|793401ed-7462-4201-9846-eaf540936914|540|\n",
      "|e0337bbc-8bc7-4ff8-8e86-e495c40dd469|2023-09-10 12:32:52|2023-09-10 12:34:52|null                                |120|\n",
      "|ee2dd754-84f2-4f5c-89da-26e1a5c6f41b|2023-09-03 17:11:44|2023-09-03 17:19:44|null                                |480|\n",
      "|bb3c489a-8358-4aca-b6cd-c016a40baa9b|2023-09-05 18:00:55|2023-09-05 18:07:55|abbcb555-a501-4333-a73e-4f147a61b65b|420|\n",
      "+------------------------------------+-------------------+-------------------+------------------------------------+---+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        duration(StartZeit, Endzeit) AS Foo\n",
    "    FROM antrag_erzeugt\n",
    "\"\"\").show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3b885-8b2d-403f-ac93-6249ede33728",
   "metadata": {},
   "source": [
    "### Übung, schreibe eine UDF, die die Dauer Cluster\n",
    "\n",
    "* alles unter 200 Sekunden \"grün\"\n",
    "* zwischen 200 und 500 \"gelb\"\n",
    "* der rest rot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a481ed8-cdbc-4e42-b79c-8caec2b5ff92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c707ff-9466-41f0-b7c5-edeeafbac132",
   "metadata": {},
   "source": [
    "## Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c97a9-dd93-431f-bb61-af38948dfbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": "<mxfile host=\"pupil-a.bin-ich-tot.de\" modified=\"2023-09-04T09:35:05.741Z\" agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\" version=\"21.1.6\" etag=\"V5d1q9Xg-VIdT2s2M7ay\" type=\"embed\">\n  <diagram id=\"zsaFwEb8lurTMNKA9gXb\" name=\"Page-1\">\n    <mxGraphModel dx=\"1036\" dy=\"570\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"827\" pageHeight=\"1169\" math=\"0\" shadow=\"0\">\n      <root>\n        <mxCell id=\"0\" />\n        <mxCell id=\"1\" parent=\"0\" />\n        <mxCell id=\"2\" value=\"\" style=\"rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"120\" y=\"120\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"3\" value=\"\" style=\"rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"294\" y=\"130\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n      </root>\n    </mxGraphModel>\n  </diagram>\n</mxfile>\n"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
